# Configuration for Hateful Memes Classification

# General settings
name: hateful_memes_classification
seed: 42
output_dir: ./output/hateful_memes

# Resume training settings
resume:
  enabled: true
  checkpoint_path: "./output/hateful_memes/best_model.pth"  # Path to saved checkpoint
  strict: true  # Whether to strictly enforce that the keys match between state_dict and model
  reset_optimizer: false  # Whether to reset the optimizer
  reset_lr_scheduler: false  # Whether to reset the learning rate scheduler
  reset_epoch: false  # Whether to reset the epoch counter

# Dataset settings
data:
  modalities: ['image', 'text']
  dataset_source: 'huggingface'  # Use 'local' or 'huggingface'
  dataset_name: 'neuralcatcher/hateful_memes'  # Hugging Face dataset name
  cache_dir: null  # Optional directory to cache dataset, null for default
  data_path: './hateful_memes'  # Path to the hateful_memes directory with image files
  train_data: 
    batch_size: 16  # Reduced batch size to avoid memory issues
    num_workers: 2  # Reduced workers to avoid potential issues
  val_data:
    batch_size: 16
    num_workers: 2
  test_data:
    batch_size: 16
    num_workers: 2
  missing_modalities:
    enabled: true
    train_missing_prob: 0.0  # No missing modalities during training
    test_missing: ['image', 'text']  # Test with each modality missing

# Image transformation settings
image_transforms:
  train:
    image_size: 224
    normalize: true
  val:
    image_size: 224
    normalize: true
  test:
    image_size: 224
    normalize: true

# Text transformation settings
text_transforms:
  model_name: "bert-base-uncased"
  max_length: 128

# Model settings
model:
  type: "hateful_memes"
  num_classes: 2  # Binary classification (hateful or not)
  image_encoder: "resnet50"
  text_encoder: "bert-base-uncased"
  fusion_type: "attention"  # Options: early, late, attention
  fusion_dim: 512
  dropout: 0.3
  pretrained: true
  handling_missing:
    strategy: "learned"  # Options: zero, mean, learned

# Training settings
training:
  epochs: 30
  criterion: "cross_entropy"  # Options: cross_entropy, bce, focal
  optimizer:
    name: "adamw"
    lr: 0.0001
    weight_decay: 0.01
  scheduler:
    name: "cosine"
    warmup_epochs: 3
    min_lr: 0.000001
  early_stopping:
    enabled: true
    patience: 5
    monitor: "val_loss"
  save_interval: 1
  log_interval: 10
  evaluation:
    metrics: ["accuracy", "precision", "recall", "f1"]

# Distributed training
distributed:
  enabled: false  # Enable only if you have multiple GPUs
  backend: "nccl"
  find_unused_parameters: true

# TensorBoard settings
tensorboard:
  enabled: true
  log_dir: "./output/hateful_memes/tensorboard"